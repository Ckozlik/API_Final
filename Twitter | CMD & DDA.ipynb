{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API pull of Twitter Data using Tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The following code was used two separate times to extract the desired fields from the Twitter API for two separate companies and write it out to a file for analysis. The actual analysis was completed in R,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://apps.twitter.com/app/14381770/keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outpath = \"/Users/christabellekozlik/Desktop/MSBA/Fall 2017/Applied Data Analysis/Python/API Project/Twitter outpath/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = 'Ooap1TJHyio2zcCfjHelhbQeo'\n",
    "consumer_secret = 'CsURxdP4otw0d0KF61126zxRl7Vuo4ykmlmSwLiEiqKKCmRIX1'\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "access_token = '921822574660616192-BPRe3fJspTvBuDMNYedC9GAxqxg3B5o'\n",
    "access_token_secret = 'mqFBsBqJl6nAjEMg2V8Nca7TQATV2gAmPwutu3YS14aCd'\n",
    "\n",
    "auth.set_access_token(access_token,access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_tweets(screen_name):\n",
    "\n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    ids = []\n",
    "\n",
    "    #make initial request for most recent tweets\n",
    "    recent_tweets = api.user_timeline(screen_name = \"ConstructConnx\",count=200)\n",
    "\n",
    "    #save most recent tweets\n",
    "    ids.extend(recent_tweets)\n",
    "\n",
    "    #save the id of the oldest tweet less one\n",
    "    historic = ids[-1].id - 1\n",
    "\n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(recent_tweets) > 0:\n",
    "        print(\"getting tweets before %s\" % (historic))\n",
    "\n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        recent_tweets = api.user_timeline(screen_name = \"ConstructConnx\",count=200,max_id=historic)\n",
    "\n",
    "        #save most recent tweets\n",
    "        ids.extend(recent_tweets)\n",
    "\n",
    "        #update the id of the oldest tweet less one\n",
    "        historic = ids[-1].id - 1\n",
    "\n",
    "        print(\"...%s tweets downloaded so far\" % (len(ids)))\n",
    "\n",
    "    #transform the tweepy tweets into a 2D array that will populate the csv\t\n",
    "    outtweets = [[tweet.id_str, tweet.created_at, tweet.text.encode(\"utf-8\"), tweet.favorite_count, tweet.retweet_count, tweet.entities.get('hashtags'), tweet.entities.get('user_mentions'), tweet.entities.get('urls') ] for tweet in ids]\n",
    "\n",
    "    #write the csv\t\n",
    "    with open(outpath + '%s_tweets.csv' % screen_name, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\",\"created_at\",\"text\",\"favorite_count\",\"retweet_count\",\"entities\",\"entities\",\"entities\"])\n",
    "        writer.writerows(outtweets)\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_tweets(\"ConstructConnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tweets(screen_name):\n",
    "\n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    ids = []\n",
    "\n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    recent_tweets = api.user_timeline(screen_name = \"DodgeData\",count=200)\n",
    "\n",
    "    #save most recent tweets\n",
    "    ids.extend(recent_tweets)\n",
    "\n",
    "    #save the id of the oldest tweet less one\n",
    "    historic = ids[-1].id - 1\n",
    "\n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(recent_tweets) > 0:\n",
    "        print(\"getting tweets before %s\" % (historic))\n",
    "\n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        recent_tweets = api.user_timeline(screen_name = \"DodgeData\",count=200,max_id=historic)\n",
    "\n",
    "        #save most recent tweets\n",
    "        ids.extend(recent_tweets)\n",
    "\n",
    "        #update the id of the oldest tweet less one\n",
    "        historic = ids[-1].id - 1\n",
    "\n",
    "        print(\"...%s tweets downloaded so far\" % (len(ids)))\n",
    "\n",
    "    #transform the tweepy tweets into a 2D array that will populate the csv\t\n",
    "    outtweets = [[tweet.id_str, tweet.created_at, tweet.text.encode(\"utf-8\"), tweet.favorite_count, tweet.retweet_count, tweet.entities.get('hashtags'), tweet.entities.get('user_mentions'), tweet.entities.get('urls') ] for tweet in ids]\n",
    "\n",
    "    #write the csv\n",
    "    with open(outpath + '%s_tweets.csv' % screen_name, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\",\"created_at\",\"text\",\"favorite_count\",\"retweet_count\",\"entities\",\"entities\",\"entities\"])\n",
    "        writer.writerows(outtweets)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_tweets(\"DodgeData\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
